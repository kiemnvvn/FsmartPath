import streamlit as st
from streamlit_gsheets import GSheetsConnection
import google.generativeai as genai
import pandas as pd
import plotly.express as px
import time

# --- 1. C·∫§U H√åNH H·ªÜ TH·ªêNG & GIAO DI·ªÜN ---
st.set_page_config(page_title="F-SmartPath Tin h·ªçc 12", layout="wide", page_icon="üéì")

st.markdown("""
    <style>
        .block-container { padding-top: 2.5rem !important; }
        .fsmart-header { 
            background-color: #F8F9FA; padding: 15px 20px; border-radius: 12px; 
            border-left: 10px solid #FF4B4B; margin-bottom: 25px; 
            box-shadow: 2px 2px 10px rgba(0,0,0,0.05);
        }
        .topic-list { line-height: 1.1; margin-bottom: 0px; padding-bottom: 2px; font-size: 0.95rem; }
        .ai-feedback { 
            background-color: #E8F0FE; padding: 15px; border-radius: 10px; 
            border-left: 5px solid #1A73E8; margin-top: 10px; margin-bottom: 15px;
        }
    </style>
""", unsafe_allow_html=True)

API_KEY = "AIzaSyAAUyXZ_zc8Ja2DP2kQrovU1CZq0DjI-30"
genai.configure(api_key=API_KEY)

def get_available_model():
    try:
        priority_models = ['gemini-1.5-flash', 'gemini-pro']
        models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        for target in priority_models:
            full_name = f"models/{target}"
            if full_name in models: return full_name
        return models[0] if models else "models/gemini-pro"
    except: return "models/gemini-pro"

if 'quiz_data' not in st.session_state:
    st.session_state.update({
        'quiz_data': [], 'current_idx': 0, 'score': 0, 
        'history': {}, 'chat_history': [], 'is_started': False,
        'model_name': get_available_model(), 'is_finished': False, 'ai_comment': ""
    })

# --- 2. K·∫æT N·ªêI D·ªÆ LI·ªÜU ---
@st.cache_data(ttl=60)
def load_data(url):
    try:
        conn = st.connection("gsheets", type=GSheetsConnection)
        df = conn.read(spreadsheet=url)
        df.columns = [str(c).strip().lower() for c in df.columns]
        return df
    except: return pd.DataFrame()

SHEET_URL = "https://docs.google.com/spreadsheets/d/1VRjMKfEFRieebvA6WGsvBSvwHbjttTsh6he74CRPO4M/edit?usp=sharing"
all_df = load_data(SHEET_URL)

# --- 3. H√ÄM AI ƒê∆ØA RA L·ªúI KHUY√äN ---
def get_ai_feedback(score, history_list):
    wrong_topics = list(set([h['topic'] for h in history_list if not h['is_correct']]))
    prompt = f"""
    B·∫°n l√† gi√°o vi√™n d·∫°y Tin h·ªçc 12. H·ªçc sinh v·ª´a ho√†n th√†nh b√†i t·∫≠p v·ªõi k·∫øt qu·∫£ {score}/100. 
    C√°c ch·ªß ƒë·ªÅ h·ªçc sinh l√†m sai: {', '.join(wrong_topics) if wrong_topics else 'Kh√¥ng c√≥ (L√†m ƒë√∫ng h·∫øt)'}.
    H√£y ƒë∆∞a ra l·ªùi nh·∫≠n x√©t ng·∫Øn g·ªçn (t·ªëi ƒëa 3 c√¢u), x√∫c t√≠ch, mang t√≠nh ƒë·ªông vi√™n v√† ƒë·ªãnh h∆∞·ªõng. 
    X∆∞ng h√¥ Th·∫ßy - Em.
    """
    try:
        model = genai.GenerativeModel(st.session_state.model_name)
        response = model.generate_content(prompt)
        return response.text
    except:
        return "Th·∫ßy khen em ƒë√£ ho√†n th√†nh b√†i luy·ªán t·∫≠p. H√£y r√† so√°t l·∫°i c√°c c√¢u sai ƒë·ªÉ n·∫Øm v·ªØng ki·∫øn th·ª©c t·ªët h∆°n nh√©!"

# --- 4. H√ÄM T·∫†O FRAME X√ÅC NH·∫¨N ƒê·ªòC L·∫¨P ---
@st.dialog("X√°c nh·∫≠n k·∫øt th√∫c b√†i l√†m")
def show_confirm_dialog():
    st.write("‚ùì **B·∫°n c√≥ ch·∫Øc ch·∫Øn k·∫øt th√∫c b√†i l√†m kh√¥ng?**")
    st.divider()
    col1, col2 = st.columns(2)
    with col1:
        if st.button("Cancel", use_container_width=True):
            st.rerun()
    
    with col2:
        if st.button("OK", use_container_width=True, type="primary"):
            st.session_state.score = sum(10 for h in st.session_state.history.values() if h['is_correct'])
            st.write("‚öôÔ∏è **AI ƒëang ph√¢n t√≠ch b√†i l√†m c·ªßa b·∫°n....**")
            # G·ªçi AI l·∫•y l·ªùi khuy√™n tr∆∞·ªõc khi chuy·ªÉn m√†n h√¨nh
            st.session_state.ai_comment = get_ai_feedback(st.session_state.score, list(st.session_state.history.values()))
            st.session_state.is_finished = True
            st.rerun()

# --- SIDEBAR (Gi·ªØ nguy√™n) ---
with st.sidebar:
    st.title("‚öôÔ∏è C·∫•u h√¨nh")
    if not all_df.empty:
        topics = ["T·∫•t c·∫£"] + sorted(all_df['topic'].dropna().unique().tolist())
        sel_topic = st.selectbox("Ch·ªçn n·ªôi dung h·ªçc t·∫≠p:", topics)
        if st.button("üöÄ B·∫Øt ƒë·∫ßu luy·ªán ƒë·ªÅ", type="primary", use_container_width=True):
            f_df = all_df if sel_topic == "T·∫•t c·∫£" else all_df[all_df['topic'] == sel_topic]
            st.session_state.quiz_data = f_df.sample(n=min(10, len(f_df))).to_dict('records')
            st.session_state.current_idx = 0; st.session_state.score = 0; st.session_state.history = {}
            st.session_state.is_started = True; st.session_state.is_finished = False; st.session_state.ai_comment = ""
            st.rerun()
    st.divider()
    st.write(f"üìû Th·∫ßy Ki·ªÉm: 0905 89 39 59")

# --- GIAO DI·ªÜN CH√çNH ---
col_main, col_chat = st.columns([1.6, 1], gap="large")

with col_main:
    st.markdown('<div class="fsmart-header"><h1><span style="color:#FF4B4B;">F</span>SmartPath</h1><p>H·ªá th·ªëng luy·ªán t·∫≠p Tin h·ªçc 12 th√¥ng minh</p></div>', unsafe_allow_html=True)
    
    if not st.session_state.is_started:
        st.info("üëã Th·∫ßy Ki·ªÉm ch√†o b·∫°n! H√£y ch·ªçn ch·ªß ƒë·ªÅ b√™n tr√°i ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
    
    elif not st.session_state.is_finished:
        q_idx = st.session_state.current_idx
        q = st.session_state.quiz_data[q_idx]
        st.write(f"**C√¢u h·ªèi {q_idx + 1} / {len(st.session_state.quiz_data)}**")
        st.progress((q_idx + 1) / len(st.session_state.quiz_data))
        
        with st.container(border=True):
            st.markdown(f"**{q.get('content')}**")
            opts = [str(q.get(f'option {i}', '')).strip() for i in ['a','b','c','d']]
            opts = [o for o in opts if o and o.lower() != 'nan']
            
            old_ans = st.session_state.history.get(q_idx, {}).get('user_ans', None)
            default_idx = opts.index(old_ans) if old_ans in opts else 0
            ans = st.radio("Ch·ªçn c√¢u tr·∫£ l·ªùi:", opts, index=default_idx, key=f"q_{q_idx}")
            
            c_back, c_next = st.columns(2)
            with c_back:
                if st.button("‚¨ÖÔ∏è C√¢u tr∆∞·ªõc", use_container_width=True) and q_idx > 0:
                    st.session_state.current_idx -= 1; st.rerun()
            
            with c_next:
                is_last = q_idx == len(st.session_state.quiz_data) - 1
                label = "‚úÖ K·∫øt th√∫c l√†m b√†i" if is_last else "Ti·∫øp theo ‚û°Ô∏è"
                if st.button(label, use_container_width=True, type="primary"):
                    st.session_state.history[q_idx] = {
                        "topic": q.get('topic', 'Chung'), "is_correct": str(ans).strip() == str(q.get('answer', '')).strip(),
                        "user_ans": ans, "correct_ans": q.get('answer', ''), "content": q.get('content'), "opts": opts
                    }
                    if is_last: show_confirm_dialog()
                    else: st.session_state.current_idx += 1; st.rerun()
    else:
        # --- M√ÄN H√åNH K·∫æT QU·∫¢ ---
        st.success(f"üéä Ho√†n th√†nh! ƒêi·ªÉm c·ªßa b·∫°n: {st.session_state.score}/100")
        
        # HI·ªÇN TH·ªä L·ªúI KHUY√äN AI
        st.markdown(f'<div class="ai-feedback"><b>üë®‚Äçüè´ L·ªùi khuy√™n:</b><br>{st.session_state.ai_comment}</div>', unsafe_allow_html=True)

        h_list = list(st.session_state.history.values())
        df_res = pd.DataFrame(h_list)
        ratio = (len(df_res[df_res['is_correct']]) / len(st.session_state.quiz_data)) * 100

        # --- D√íNG 190: TH√äM T·∫¢I B√ÄI L√ÄM (PDF/TXT) ---
        report_text = f"B√ÅO C√ÅO K·∫æT QU·∫¢ F-SMARTPATH\nƒêi·ªÉm: {st.session_state.score}/100\nL·ªùi khuy√™n: {st.session_state.ai_comment}\n\nCHI TI·∫æT B√ÄI L√ÄM:\n"
        for i, h in enumerate(h_list):
            report_text += f"C√¢u {i+1}: {'ƒê√∫ng' if h['is_correct'] else 'Sai'}\n- N·ªôi dung: {h['content']}\n- B·∫°n ch·ªçn: {h['user_ans']}\n- ƒê√°p √°n ƒë√∫ng: {h['correct_ans']}\n\n"
        st.download_button(label="üì• T·∫£i v·ªÅ b√°o c√°o k·∫øt qu·∫£ b√†i l√†m", data=report_text, file_name=f"KetQua_FSmartPath.txt", mime="text/plain", use_container_width=True)

        c1, c2 = st.columns(2)
        with c1:
            st.plotly_chart(px.pie(df_res, names='is_correct', hole=0.4, height=300, color='is_correct', 
                         color_discrete_map={True:'#2ecc71', False:'#e74c3c'}, labels={True:'ƒê√∫ng', False:'Sai'}), use_container_width=True)
        with c2:
            st.write("üí° **Nh·∫≠n x√©t nƒÉng l·ª±c**")
            if ratio >= 90: st.success(f"üöÄ **NƒÉng l·ª±c: XU·∫§T S·∫ÆC**")
            elif ratio >= 80: st.success(f"üåü **NƒÉng l·ª±c: GI·ªéI**")
            elif ratio >= 70: st.info(f"üìà **NƒÉng l·ª±c: TRUNG B√åNH KH√Å**")
            else: st.error(f"‚ö†Ô∏è **NƒÉng l·ª±c: CH∆ØA ƒê·∫†T**")

            wrong_topics = df_res[df_res['is_correct'] == False]['topic'].unique().tolist()
            if wrong_topics:
                st.write("---")
                st.warning("üìç **C√°c ch·ªß ƒë·ªÅ c·∫ßn ch√∫ tr·ªçng:**")
                for topic in wrong_topics:
                    st.markdown(f'<p class="topic-list">- {topic}</p>', unsafe_allow_html=True)

        st.divider()
        st.subheader("üîç B·∫¢NG CHI TI·∫æT B√ÄI L√ÄM")
        view_mode = st.radio("L·ªçc hi·ªÉn th·ªã:", ["T·∫•t c·∫£", "C√¢u ƒê√∫ng", "C√¢u Sai"], horizontal=True)
        for i, h in enumerate(h_list):
            res_str = "ƒê√∫ng" if h['is_correct'] else "Sai"
            if view_mode == "T·∫•t c·∫£" or res_str == view_mode.replace("C√¢u ", ""):
                with st.expander(f"C√¢u {i+1}: {h['content'][:60]}... ({res_str})", expanded=(res_str=="Sai")):
                    st.write(f"**{h['content']}**")
                    for o in h['opts']:
                        if o == h['correct_ans']: st.write(f"‚úÖ **{o}** (ƒê√∫ng)")
                        elif o == h['user_ans'] and not h['is_correct']: st.write(f"‚ùå **{o}** (B·∫°n ch·ªçn)")
                        else: st.write(f"‚ö™ {o}")

        
        if st.button("üîÑ L√†m b√†i m·ªõi", use_container_width=True):
            st.session_state.is_started = False; st.rerun()


# --- 6. C·ªòT TR·ª¢ GI·∫¢NG AI (C·∫¨P NH·∫¨T HI·ªÜU ·ª®NG CH·ªú) ---
with col_chat:
    st.markdown('<div class="assistant-header">ü§ñ AI Mentor</div>', unsafe_allow_html=True)
    chat_box = st.container(height=450, border=True)
    
    with chat_box:
        for msg in st.session_state.chat_history:
            with st.chat_message(msg["role"]): st.markdown(msg["content"])
            
    if prompt := st.chat_input("H·ªèi AI..."):
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        with chat_box:
            st.chat_message("user").markdown(prompt)
            
            # --- HI·ªÜU ·ª®NG AI ƒêANG SUY NGHƒ® ---
            with st.chat_message("assistant"):
                thinking_placeholder = st.empty()
                status_text = "AI ƒëang suy nghƒ©"
                
                # B·∫Øt ƒë·∫ßu g·ªçi API v√† ch·∫°y hi·ªáu ·ª©ng ch·ªØ
                try:
                    model = genai.GenerativeModel(st.session_state.model_name)
                    
                    # S·ª≠ d·ª•ng stream=True ƒë·ªÉ t·∫°o c·∫£m gi√°c ph·∫£n h·ªìi nhanh ho·∫∑c gi·∫£ l·∫≠p b·∫±ng v√≤ng l·∫∑p
                    # ·ªû ƒë√¢y ta d√πng v√≤ng l·∫∑p ƒë∆°n gi·∫£n ƒë·ªÉ hi·ªán d·∫•u ch·∫•m ƒë·ªông trong l√∫c API x·ª≠ l√Ω
                    with st.spinner(''):
                        # Hi·ªáu ·ª©ng l·∫∑p k√Ω t·ª± d·∫•u ch·∫•m
                        for i in range(3):
                            for dots in [".", "..", "..."]:
                                thinking_placeholder.markdown(f"*{status_text}{dots}*")
                                time.sleep(0.3)
                        
                        # G·ªçi k·∫øt qu·∫£ th·ª±c t·∫ø
                        response = model.generate_content(f"Gi·∫£i th√≠ch ng·∫Øn g·ªçn cho h·ªçc sinh 12: {prompt}")
                        thinking_placeholder.markdown(response.text)
                        st.session_state.chat_history.append({"role": "assistant", "content": response.text})
                except:
                    thinking_placeholder.error("D·ªãch v·ª• AI ƒëang b·∫≠n, th·∫ßy h√£y th·ª≠ l·∫°i nh√©!")